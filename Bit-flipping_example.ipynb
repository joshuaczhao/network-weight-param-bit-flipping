{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in dataset, model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load in data and such\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "# CIFAR10 images are 3x32x32, 3-channel 32x32 images\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, train_drop1, train_drop2, train_drop3, train_drop4, train_drop5):    \n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 3, padding=1)   # (in-channels, out-channels, kernel size)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(train_drop1)\n",
    "        self.conv2 = nn.Conv2d(12, 32, 3, padding=1)\n",
    "        self.dropout2 = nn.Dropout(train_drop2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.dropout3 = nn.Dropout(train_drop3)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.dropout4 = nn.Dropout(train_drop4)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.dropout5 = nn.Dropout(train_drop5)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x, bool, drop1, drop2, drop3, drop4, drop5):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout1(x)        \n",
    "        x = F.dropout(x, drop1, bool)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.dropout(x, drop2, bool)\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.reshape(-1, 64 * 4 * 4)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.dropout(x, drop3, bool)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.dropout(x, drop4, bool)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = F.dropout(x, drop5, bool)\n",
    "    \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example loading in a network then dynamically quantizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0, inplace=False)\n",
      "  (conv2): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dropout2): Dropout(p=0, inplace=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dropout3): Dropout(p=0, inplace=False)\n",
      "  (fc1): DynamicQuantizedLinear(in_features=1024, out_features=128, qscheme=torch.per_tensor_affine)\n",
      "  (dropout4): Dropout(p=0, inplace=False)\n",
      "  (fc2): DynamicQuantizedLinear(in_features=128, out_features=128, qscheme=torch.per_tensor_affine)\n",
      "  (dropout5): Dropout(p=0, inplace=False)\n",
      "  (fc3): DynamicQuantizedLinear(in_features=128, out_features=10, qscheme=torch.per_tensor_affine)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "PATH = './sample_models/cifar_net_sgd_72_41.pth'\n",
    "\n",
    "quant_net = Net(train_drop1=0, train_drop2=0, train_drop3=0, train_drop4=0, train_drop5=0)\n",
    "quant_net.load_state_dict(torch.load(PATH))\n",
    "quant_net = torch.quantization.quantize_dynamic(quant_net, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "print(quant_net)  # Can see here that only the linear layers have been quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping one bit at a time (w/ 0.5 rate). Iterating between which bit to flip to check accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cifar_net_sgd_72_41.pth\n",
      "\n",
      "Unmodded Model Accuracy: 72.35 %\n",
      "\n",
      "Bit value: 0 254\n",
      "\n",
      "Modified Accuracy: 71.91 %\n",
      "Modified Accuracy: 71.90 %\n",
      "Modified Accuracy: 71.87 %\n",
      "Modified Accuracy: 71.90 %\n",
      "Modified Accuracy: 71.89 %\n",
      "Modified Accuracy: 71.88 %\n",
      "Modified Accuracy: 71.91 %\n",
      "Modified Accuracy: 71.93 %\n",
      "Modified Accuracy: 72.01 %\n",
      "Modified Accuracy: 71.90 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 71.91 %\n",
      "\n",
      "\n",
      "Bit value: 1 253\n",
      "\n",
      "Modified Accuracy: 71.30 %\n",
      "Modified Accuracy: 71.18 %\n",
      "Modified Accuracy: 71.12 %\n",
      "Modified Accuracy: 71.16 %\n",
      "Modified Accuracy: 71.25 %\n",
      "Modified Accuracy: 71.15 %\n",
      "Modified Accuracy: 71.09 %\n",
      "Modified Accuracy: 71.14 %\n",
      "Modified Accuracy: 71.13 %\n",
      "Modified Accuracy: 71.23 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 71.17 %\n",
      "\n",
      "\n",
      "Bit value: 2 251\n",
      "\n",
      "Modified Accuracy: 69.30 %\n",
      "Modified Accuracy: 69.28 %\n",
      "Modified Accuracy: 69.42 %\n",
      "Modified Accuracy: 69.27 %\n",
      "Modified Accuracy: 69.10 %\n",
      "Modified Accuracy: 69.22 %\n",
      "Modified Accuracy: 69.24 %\n",
      "Modified Accuracy: 69.25 %\n",
      "Modified Accuracy: 69.57 %\n",
      "Modified Accuracy: 69.29 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 69.29 %\n",
      "\n",
      "\n",
      "Bit value: 3 247\n",
      "\n",
      "Modified Accuracy: 61.71 %\n",
      "Modified Accuracy: 61.33 %\n",
      "Modified Accuracy: 61.72 %\n",
      "Modified Accuracy: 62.00 %\n",
      "Modified Accuracy: 61.35 %\n",
      "Modified Accuracy: 62.17 %\n",
      "Modified Accuracy: 61.82 %\n",
      "Modified Accuracy: 61.34 %\n",
      "Modified Accuracy: 61.65 %\n",
      "Modified Accuracy: 61.68 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 61.68 %\n",
      "\n",
      "\n",
      "Bit value: 4 239\n",
      "\n",
      "Modified Accuracy: 29.08 %\n",
      "Modified Accuracy: 28.32 %\n",
      "Modified Accuracy: 27.79 %\n",
      "Modified Accuracy: 28.26 %\n",
      "Modified Accuracy: 28.48 %\n",
      "Modified Accuracy: 28.04 %\n",
      "Modified Accuracy: 27.84 %\n",
      "Modified Accuracy: 29.61 %\n",
      "Modified Accuracy: 29.16 %\n",
      "Modified Accuracy: 28.58 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 28.52 %\n",
      "\n",
      "\n",
      "Bit value: 5 223\n",
      "\n",
      "Modified Accuracy: 11.22 %\n",
      "Modified Accuracy: 10.80 %\n",
      "Modified Accuracy: 10.47 %\n",
      "Modified Accuracy: 11.10 %\n",
      "Modified Accuracy: 10.81 %\n",
      "Modified Accuracy: 10.73 %\n",
      "Modified Accuracy: 10.59 %\n",
      "Modified Accuracy: 10.62 %\n",
      "Modified Accuracy: 11.08 %\n",
      "Modified Accuracy: 10.89 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 10.83 %\n",
      "\n",
      "\n",
      "Bit value: 6 191\n",
      "\n",
      "Modified Accuracy: 10.02 %\n",
      "Modified Accuracy: 10.06 %\n",
      "Modified Accuracy: 10.02 %\n",
      "Modified Accuracy: 10.03 %\n",
      "Modified Accuracy: 10.05 %\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.01 %\n",
      "Modified Accuracy: 10.05 %\n",
      "Modified Accuracy: 10.01 %\n",
      "Modified Accuracy: 10.02 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 10.03 %\n",
      "\n",
      "\n",
      "Bit value: 7 127\n",
      "\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.00 %\n",
      "Modified Accuracy: 10.00 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 10.00 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = './sample_models/cifar_net_sgd_72_41.pth'\n",
    "print(PATH)\n",
    "print(\"\")\n",
    "\n",
    "# print(quant_net)  # Can see here that only the linear layers have been quantized\n",
    "\n",
    "fc_layers = [\"fc1._packed_params.weight\", \"fc2._packed_params.weight\", \"fc3._packed_params.weight\"]\n",
    "\n",
    "bit_values = [254, 253, 251, 247, 239, 223, 191, 127]\n",
    "\n",
    "\n",
    "# Load in unmodded quant net for accuracy comparison ----------------------------------------------------------------------------------------------\n",
    "unmod_quant_net = Net(train_drop1=0, train_drop2=0, train_drop3=0, train_drop4=0, train_drop5=0)\n",
    "unmod_quant_net.load_state_dict(torch.load(PATH))\n",
    "unmod_quant_net = torch.quantization.quantize_dynamic(unmod_quant_net, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "correct = 0\n",
    "total = 0 \n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = unmod_quant_net(images, False, drop1=0, drop2=0, drop3=0, drop4=0, drop5=0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Unmodded Model Accuracy: %.2f %%' % (100 * correct / float(total)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "for j in range(len(bit_values)):\n",
    "    print(\"Bit value:\", j, bit_values[j])\n",
    "    print(\"\")\n",
    "    ave_sum = 0\n",
    "    loops = 10\n",
    "    \n",
    "    for k in range(loops):\n",
    "        \n",
    "        quant_net = Net(train_drop1=0, train_drop2=0, train_drop3=0, train_drop4=0, train_drop5=0)\n",
    "        quant_net.load_state_dict(torch.load(PATH))\n",
    "        quant_net = torch.quantization.quantize_dynamic(quant_net, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "    \n",
    "        for i in range(len(fc_layers)):\n",
    "        #     print(fc_layers[i])\n",
    "        #     print(\"\")\n",
    "\n",
    "            fc_weights = quant_net.state_dict()[fc_layers[i]]\n",
    "            fc_weights_int = quant_net.state_dict()[fc_layers[i]].int_repr()\n",
    "\n",
    "            scale = fc_weights.q_scale()\n",
    "            zero_pt = fc_weights.q_zero_point()\n",
    "\n",
    "        #     print(fc_weights)\n",
    "            # print(fc_weights_int)\n",
    "            # print(fc_weights_int.max(), fc_weights_int.min())\n",
    "\n",
    "            # ------------------------------------ BIT FLIPPING IS HERE. MODIFY random_arr VALUES AND THRESHOLD TO CHANGE -------------------------\n",
    "\n",
    "            fc_s = fc_weights.size()\n",
    "            random_arr = torch.rand(fc_s)\n",
    "\n",
    "            # print(random_arr)\n",
    "\n",
    "            # convert based on threshold value (value determines how often bit is flipped from 1-0)\n",
    "            threshold_v = 0.5\n",
    "            threshold = 1 - threshold_v\n",
    "\n",
    "            # BIT MASKING\n",
    "    #         random_arr[random_arr >= threshold] = 127   # MSB\n",
    "    #         random_arr[random_arr >= threshold] = 254   # LSB\n",
    "            random_arr[random_arr >= threshold] = bit_values[j]\n",
    "            random_arr[random_arr < threshold] = 255\n",
    "            random_arr_np = random_arr.type(torch.uint8)\n",
    "\n",
    "            new_fc = fc_weights_int.type(torch.uint8)\n",
    "\n",
    "            new_arr = torch.bitwise_and(random_arr_np, new_fc).type(torch.int8)\n",
    "\n",
    "            '''\n",
    "            # Same stuff as above, just done with numpy arrays here\n",
    "            random_arr_np = random_arr.type(torch.uint8).numpy()\n",
    "\n",
    "            # print(random_arr_np)\n",
    "\n",
    "            new_fc = fc_weights_int.type(torch.uint8).numpy().astype('uint8')\n",
    "            # print(new_fc)\n",
    "\n",
    "            new_arr = torch.tensor(np.bitwise_and(random_arr_np, new_fc).astype('int8'))\n",
    "            # print(new_arr)\n",
    "            '''\n",
    "\n",
    "\n",
    "            # --------------------------------Reload new values (or same ones) into the network again --------------------------------\n",
    "\n",
    "            manual = (new_arr-zero_pt)*scale\n",
    "\n",
    "            new_quant_tensor = torch.quantize_per_tensor(manual, scale, zero_pt, torch.qint8)\n",
    "\n",
    "            state_dict = quant_net.state_dict()\n",
    "            state_dict[fc_layers[i]] = new_quant_tensor\n",
    "            quant_net.load_state_dict(state_dict)\n",
    "\n",
    "            c1_w_params = quant_net.state_dict()[fc_layers[i]]\n",
    "\n",
    "        #     print(c1_w_params)\n",
    "\n",
    "        #     print(\"\")\n",
    "        #     print(\"Sum before:\", torch.sum(fc_weights_int).item(), \"  Sum after:\", torch.sum(new_arr).item())\n",
    "\n",
    "        #     print(\"\")\n",
    "        #     print(\"\")\n",
    "        #     print(\"\")\n",
    "\n",
    "\n",
    "        # Now check accuracy ---------------------------------------------------------------------------------------------\n",
    "\n",
    "        correct = 0\n",
    "        total = 0 \n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                outputs = quant_net(images, False, drop1=0, drop2=0, drop3=0, drop4=0, drop5=0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print('Modified Accuracy: %.2f %%' % (100 * correct / float(total)))\n",
    "    \n",
    "        ave_sum += 100 * correct / float(total)\n",
    "        \n",
    "    print(\"\")\n",
    "    print('Number of loops:', loops)\n",
    "    print('Average mod acc is: %.2f %%' % (ave_sum/float(loops)))\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping 4 bits at the same time (first 4 or last 4 or none flipped) [00001111 or 11110000 or 11111111 mask used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cifar_weight_2_1e-3.pth\n",
      "\n",
      "Unmodded Model Accuracy: 69.87 %\n",
      "\n",
      "Top_p 0 Bot_p 0.5\n",
      "\n",
      "Modified Accuracy: 63.10 %\n",
      "Modified Accuracy: 63.16 %\n",
      "Modified Accuracy: 62.18 %\n",
      "Modified Accuracy: 61.60 %\n",
      "Modified Accuracy: 61.89 %\n",
      "Modified Accuracy: 62.50 %\n",
      "Modified Accuracy: 63.24 %\n",
      "Modified Accuracy: 62.78 %\n",
      "Modified Accuracy: 63.01 %\n",
      "Modified Accuracy: 62.05 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 62.55 %\n",
      "\n",
      "\n",
      "Top_p 0.1 Bot_p 0.4\n",
      "\n",
      "Modified Accuracy: 59.69 %\n",
      "Modified Accuracy: 61.83 %\n",
      "Modified Accuracy: 60.68 %\n",
      "Modified Accuracy: 60.45 %\n",
      "Modified Accuracy: 62.72 %\n",
      "Modified Accuracy: 61.25 %\n",
      "Modified Accuracy: 62.12 %\n",
      "Modified Accuracy: 65.33 %\n",
      "Modified Accuracy: 60.68 %\n",
      "Modified Accuracy: 59.27 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 61.40 %\n",
      "\n",
      "\n",
      "Top_p 0.2 Bot_p 0.30000000000000004\n",
      "\n",
      "Modified Accuracy: 56.91 %\n",
      "Modified Accuracy: 55.01 %\n",
      "Modified Accuracy: 58.31 %\n",
      "Modified Accuracy: 62.42 %\n",
      "Modified Accuracy: 57.67 %\n",
      "Modified Accuracy: 59.54 %\n",
      "Modified Accuracy: 58.90 %\n",
      "Modified Accuracy: 61.33 %\n",
      "Modified Accuracy: 65.68 %\n",
      "Modified Accuracy: 62.71 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 59.85 %\n",
      "\n",
      "\n",
      "Top_p 0.3 Bot_p 0.19999999999999996\n",
      "\n",
      "Modified Accuracy: 56.42 %\n",
      "Modified Accuracy: 59.56 %\n",
      "Modified Accuracy: 48.39 %\n",
      "Modified Accuracy: 53.54 %\n",
      "Modified Accuracy: 50.73 %\n",
      "Modified Accuracy: 45.80 %\n",
      "Modified Accuracy: 51.62 %\n",
      "Modified Accuracy: 50.79 %\n",
      "Modified Accuracy: 55.63 %\n",
      "Modified Accuracy: 49.56 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 52.20 %\n",
      "\n",
      "\n",
      "Top_p 0.4 Bot_p 0.09999999999999998\n",
      "\n",
      "Modified Accuracy: 39.62 %\n",
      "Modified Accuracy: 46.16 %\n",
      "Modified Accuracy: 29.77 %\n",
      "Modified Accuracy: 38.49 %\n",
      "Modified Accuracy: 37.42 %\n",
      "Modified Accuracy: 35.89 %\n",
      "Modified Accuracy: 20.69 %\n",
      "Modified Accuracy: 36.14 %\n",
      "Modified Accuracy: 35.46 %\n",
      "Modified Accuracy: 24.79 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 34.44 %\n",
      "\n",
      "\n",
      "Top_p 0.5 Bot_p 0.0\n",
      "\n",
      "Modified Accuracy: 17.53 %\n",
      "Modified Accuracy: 21.37 %\n",
      "Modified Accuracy: 23.97 %\n",
      "Modified Accuracy: 31.00 %\n",
      "Modified Accuracy: 23.90 %\n",
      "Modified Accuracy: 27.88 %\n",
      "Modified Accuracy: 31.32 %\n",
      "Modified Accuracy: 34.04 %\n",
      "Modified Accuracy: 17.60 %\n",
      "Modified Accuracy: 25.21 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 25.38 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the quantized weights for fc (fc1) layer ----------------------------------------------------------------------------\n",
    "\n",
    "PATH = './sample_models/cifar_weight_2_1e-3.pth'\n",
    "print(PATH)\n",
    "print(\"\")\n",
    "\n",
    "# print(quant_net)  # Can see here that only the linear layers have been quantized\n",
    "\n",
    "fc_layers = [\"fc1._packed_params.weight\", \"fc2._packed_params.weight\", \"fc3._packed_params.weight\"]\n",
    "\n",
    "bit_values = [15, 240]  ## 00001111, 11110000\n",
    "\n",
    "top_p = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Load in unmodded for comparison ----------------------------------------------------------------------------------------------\n",
    "unmod_quant_net = Net(train_drop1=0, train_drop2=0, train_drop3=0, train_drop4=0, train_drop5=0)\n",
    "unmod_quant_net.load_state_dict(torch.load(PATH))\n",
    "unmod_quant_net = torch.quantization.quantize_dynamic(unmod_quant_net, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "correct = 0\n",
    "total = 0 \n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = unmod_quant_net(images, False, drop1=0, drop2=0, drop3=0, drop4=0, drop5=0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Unmodded Model Accuracy: %.2f %%' % (100 * correct / float(total)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(top_p)):\n",
    "    print(\"Top_p\", top_p[j], \"Bot_p\", 1-top_p[j]-0.5)\n",
    "    print(\"\")\n",
    "    ave_sum = 0\n",
    "    loops = 10\n",
    "    \n",
    "    for k in range(loops):\n",
    "        \n",
    "        quant_net = Net(train_drop1=0, train_drop2=0, train_drop3=0, train_drop4=0, train_drop5=0)\n",
    "        quant_net.load_state_dict(torch.load(PATH))\n",
    "        quant_net = torch.quantization.quantize_dynamic(quant_net, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "    \n",
    "        for i in range(len(fc_layers)):\n",
    "        #     print(fc_layers[i])\n",
    "        #     print(\"\")\n",
    "\n",
    "            fc_weights = quant_net.state_dict()[fc_layers[i]]\n",
    "            fc_weights_int = quant_net.state_dict()[fc_layers[i]].int_repr()\n",
    "\n",
    "            scale = fc_weights.q_scale()\n",
    "            zero_pt = fc_weights.q_zero_point()\n",
    "\n",
    "            # ------------------------------------Insert some bit flipping stuff here ------------------------------------------------\n",
    "            fc_s = fc_weights.size()\n",
    "            random_arr = torch.rand(fc_s)\n",
    "\n",
    "            # convert based on threshold value (value determines how often bit is flipped from 1-0)\n",
    "            threshold_v = 0.5\n",
    "            threshold = 1 - threshold_v\n",
    "            \n",
    "            top_v = 1 - top_p[j]\n",
    "\n",
    "            # Bit masking\n",
    "            random_arr[(random_arr > threshold) & (random_arr > top_v)] = bit_values[0]\n",
    "            random_arr[(random_arr > threshold) & (random_arr <= top_v)] = bit_values[1]\n",
    "            random_arr[random_arr <= threshold] = 255\n",
    "            random_arr_np = random_arr.type(torch.uint8)\n",
    "\n",
    "            new_fc = fc_weights_int.type(torch.uint8)\n",
    "\n",
    "            new_arr = torch.bitwise_and(random_arr_np, new_fc).type(torch.int8)\n",
    "\n",
    "\n",
    "            # --------------------------------Reload new values (or same ones) into the network again --------------------------------\n",
    "\n",
    "            manual = (new_arr-zero_pt)*scale\n",
    "\n",
    "            new_quant_tensor = torch.quantize_per_tensor(manual, scale, zero_pt, torch.qint8)\n",
    "\n",
    "            state_dict = quant_net.state_dict()\n",
    "            state_dict[fc_layers[i]] = new_quant_tensor\n",
    "            quant_net.load_state_dict(state_dict)\n",
    "\n",
    "            c1_w_params = quant_net.state_dict()[fc_layers[i]]\n",
    "\n",
    "\n",
    "        # Now check accuracy ---------------------------------------------------------------------------------------------\n",
    "\n",
    "        correct = 0\n",
    "        total = 0 \n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                outputs = quant_net(images, False, drop1=0, drop2=0, drop3=0, drop4=0, drop5=0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print('Modified Accuracy: %.2f %%' % (100 * correct / float(total)))\n",
    "    \n",
    "        ave_sum += 100 * correct / float(total)\n",
    "        \n",
    "    print(\"\")\n",
    "    print('Number of loops:', loops)\n",
    "    print('Average mod acc is: %.2f %%' % (ave_sum/float(loops)))\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only 1 bit can be flipped at a time, 0.5 rate (randomly pick one in upper 4 or lower 4 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cifar_weight_2_1e-3.pth\n",
      "\n",
      "Unmodded Model Accuracy: 69.87 %\n",
      "\n",
      "Flipping lower bits\n",
      "\n",
      "Modified Accuracy: 68.60 %\n",
      "Modified Accuracy: 68.94 %\n",
      "Modified Accuracy: 69.29 %\n",
      "Modified Accuracy: 68.83 %\n",
      "Modified Accuracy: 68.80 %\n",
      "Modified Accuracy: 69.32 %\n",
      "Modified Accuracy: 69.37 %\n",
      "Modified Accuracy: 68.75 %\n",
      "Modified Accuracy: 69.23 %\n",
      "Modified Accuracy: 68.64 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 68.98 %\n",
      "\n",
      "\n",
      "Flipping upper bits\n",
      "\n",
      "Modified Accuracy: 26.14 %\n",
      "Modified Accuracy: 19.04 %\n",
      "Modified Accuracy: 19.18 %\n",
      "Modified Accuracy: 23.95 %\n",
      "Modified Accuracy: 29.09 %\n",
      "Modified Accuracy: 19.32 %\n",
      "Modified Accuracy: 28.46 %\n",
      "Modified Accuracy: 19.58 %\n",
      "Modified Accuracy: 20.93 %\n",
      "Modified Accuracy: 22.41 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 22.81 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the quantized weights for fc (fc1) layer ----------------------------------------------------------------------------\n",
    "\n",
    "PATH = './sample_models/cifar_weight_2_1e-3.pth'\n",
    "print(PATH)\n",
    "print(\"\")\n",
    "\n",
    "# print(quant_net)  # Can see here that only the linear layers have been quantized\n",
    "\n",
    "fc_layers = [\"fc1._packed_params.weight\", \"fc2._packed_params.weight\", \"fc3._packed_params.weight\"]\n",
    "\n",
    "bit_values_least = [254, 253, 251, 247]\n",
    "bit_values_most  = [239, 223, 191, 127]\n",
    "\n",
    "# Load in unmodded for comparison ----------------------------------------------------------------------------------------------\n",
    "unmod_quant_net = Net(train_drop1=0, train_drop2=0, train_drop3=0, train_drop4=0, train_drop5=0)\n",
    "unmod_quant_net.load_state_dict(torch.load(PATH))\n",
    "unmod_quant_net = torch.quantization.quantize_dynamic(unmod_quant_net, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "correct = 0\n",
    "total = 0 \n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = unmod_quant_net(images, False, drop1=0, drop2=0, drop3=0, drop4=0, drop5=0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Unmodded Model Accuracy: %.2f %%' % (100 * correct / float(total)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "for j in range(2):\n",
    "    if j == 0:\n",
    "        print(\"Flipping lower bits\")\n",
    "    elif j == 1:\n",
    "        print(\"Flipping upper bits\")\n",
    "    else:\n",
    "        print(\"ERROR!!!!!\")\n",
    "        break\n",
    "    print(\"\")\n",
    "    ave_sum = 0\n",
    "    loops = 10\n",
    "    \n",
    "    for k in range(loops):\n",
    "        \n",
    "        quant_net = Net(train_drop1=0, train_drop2=0, train_drop3=0, train_drop4=0, train_drop5=0)\n",
    "        quant_net.load_state_dict(torch.load(PATH))\n",
    "        quant_net = torch.quantization.quantize_dynamic(quant_net, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "    \n",
    "        for i in range(len(fc_layers)):\n",
    "        #     print(fc_layers[i])\n",
    "        #     print(\"\")\n",
    "\n",
    "            fc_weights = quant_net.state_dict()[fc_layers[i]]\n",
    "            fc_weights_int = quant_net.state_dict()[fc_layers[i]].int_repr()\n",
    "\n",
    "            scale = fc_weights.q_scale()\n",
    "            zero_pt = fc_weights.q_zero_point()\n",
    "\n",
    "            # ------------------------------------Insert some bit flipping stuff here ------------------------------------------------\n",
    "            fc_s = fc_weights.size()\n",
    "            random_arr = torch.rand(fc_s)\n",
    "\n",
    "            # convert based on threshold value (value determines how often bit is flipped from 1-0)\n",
    "            threshold_v = 0.5\n",
    "            threshold = 1 - threshold_v\n",
    "            \n",
    "            # Manually 1/4 chance for each bit to flip\n",
    "            t1 = 1 - 0.125\n",
    "            t2 = 1 - 0.25\n",
    "            t3 = 1 - 0.375\n",
    "            t4 = 1 - 0.5\n",
    "            \n",
    "\n",
    "            # Bit masking\n",
    "    #         random_arr[random_arr >= threshold] = 127   # MSB\n",
    "    #         random_arr[random_arr >= threshold] = 254   # LSB\n",
    "    \n",
    "            if j == 0:\n",
    "                random_arr[random_arr > t1] = bit_values_least[0]\n",
    "                random_arr[(random_arr > t2) & (random_arr <= t1)] = bit_values_least[1]\n",
    "                random_arr[(random_arr > t3) & (random_arr <= t2)] = bit_values_least[2]\n",
    "                random_arr[(random_arr > t4) & (random_arr <= t3)] = bit_values_least[3]\n",
    "            elif j == 1:\n",
    "                random_arr[random_arr > t1] = bit_values_most[0]\n",
    "                random_arr[(random_arr > t2) & (random_arr <= t1)] = bit_values_most[1]\n",
    "                random_arr[(random_arr > t3) & (random_arr <= t2)] = bit_values_most[2]\n",
    "                random_arr[(random_arr > t4) & (random_arr <= t3)] = bit_values_most[3]\n",
    "            \n",
    "            random_arr[random_arr <= threshold] = 255\n",
    "            random_arr_np = random_arr.type(torch.uint8)\n",
    "\n",
    "            new_fc = fc_weights_int.type(torch.uint8)\n",
    "\n",
    "            new_arr = torch.bitwise_and(random_arr_np, new_fc).type(torch.int8)\n",
    "\n",
    "\n",
    "            # --------------------------------Reload new values (or same ones) into the network again --------------------------------\n",
    "\n",
    "            manual = (new_arr-zero_pt)*scale\n",
    "\n",
    "            new_quant_tensor = torch.quantize_per_tensor(manual, scale, zero_pt, torch.qint8)\n",
    "\n",
    "            state_dict = quant_net.state_dict()\n",
    "            state_dict[fc_layers[i]] = new_quant_tensor\n",
    "            quant_net.load_state_dict(state_dict)\n",
    "\n",
    "            c1_w_params = quant_net.state_dict()[fc_layers[i]]\n",
    "\n",
    "\n",
    "        # Now check accuracy ---------------------------------------------------------------------------------------------\n",
    "\n",
    "        correct = 0\n",
    "        total = 0 \n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                outputs = quant_net(images, False, drop1=0, drop2=0, drop3=0, drop4=0, drop5=0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print('Modified Accuracy: %.2f %%' % (100 * correct / float(total)))\n",
    "    \n",
    "        ave_sum += 100 * correct / float(total)\n",
    "        \n",
    "    print(\"\")\n",
    "    print('Number of loops:', loops)\n",
    "    print('Average mod acc is: %.2f %%' % (ave_sum/float(loops)))\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipping 1 random bit in upper and lower at same time at different rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cifar_weight_2_1e-3.pth\n",
      "\n",
      "Unmodded Model Accuracy: 69.87 %\n",
      "\n",
      "Top_p 0 Bot_p 0.5\n",
      "\n",
      "Modified Accuracy: 69.42 %\n",
      "Modified Accuracy: 69.36 %\n",
      "Modified Accuracy: 68.63 %\n",
      "Modified Accuracy: 68.91 %\n",
      "Modified Accuracy: 69.13 %\n",
      "Modified Accuracy: 68.76 %\n",
      "Modified Accuracy: 69.22 %\n",
      "Modified Accuracy: 69.18 %\n",
      "Modified Accuracy: 69.44 %\n",
      "Modified Accuracy: 69.17 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 69.12 %\n",
      "\n",
      "\n",
      "Top_p 0.1 Bot_p 0.4\n",
      "\n",
      "Modified Accuracy: 64.66 %\n",
      "Modified Accuracy: 57.92 %\n",
      "Modified Accuracy: 55.22 %\n",
      "Modified Accuracy: 64.95 %\n",
      "Modified Accuracy: 63.27 %\n",
      "Modified Accuracy: 61.56 %\n",
      "Modified Accuracy: 62.50 %\n",
      "Modified Accuracy: 65.21 %\n",
      "Modified Accuracy: 59.78 %\n",
      "Modified Accuracy: 61.16 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 61.62 %\n",
      "\n",
      "\n",
      "Top_p 0.2 Bot_p 0.30000000000000004\n",
      "\n",
      "Modified Accuracy: 49.38 %\n",
      "Modified Accuracy: 45.30 %\n",
      "Modified Accuracy: 54.03 %\n",
      "Modified Accuracy: 54.48 %\n",
      "Modified Accuracy: 52.58 %\n",
      "Modified Accuracy: 48.54 %\n",
      "Modified Accuracy: 46.86 %\n",
      "Modified Accuracy: 37.92 %\n",
      "Modified Accuracy: 47.62 %\n",
      "Modified Accuracy: 54.61 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 49.13 %\n",
      "\n",
      "\n",
      "Top_p 0.3 Bot_p 0.19999999999999996\n",
      "\n",
      "Modified Accuracy: 38.02 %\n",
      "Modified Accuracy: 48.17 %\n",
      "Modified Accuracy: 40.88 %\n",
      "Modified Accuracy: 38.89 %\n",
      "Modified Accuracy: 41.33 %\n",
      "Modified Accuracy: 40.43 %\n",
      "Modified Accuracy: 33.72 %\n",
      "Modified Accuracy: 41.93 %\n",
      "Modified Accuracy: 36.21 %\n",
      "Modified Accuracy: 30.57 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 39.01 %\n",
      "\n",
      "\n",
      "Top_p 0.4 Bot_p 0.09999999999999998\n",
      "\n",
      "Modified Accuracy: 38.04 %\n",
      "Modified Accuracy: 34.50 %\n",
      "Modified Accuracy: 36.02 %\n",
      "Modified Accuracy: 15.73 %\n",
      "Modified Accuracy: 35.02 %\n",
      "Modified Accuracy: 25.86 %\n",
      "Modified Accuracy: 21.39 %\n",
      "Modified Accuracy: 32.85 %\n",
      "Modified Accuracy: 23.32 %\n",
      "Modified Accuracy: 20.05 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 28.28 %\n",
      "\n",
      "\n",
      "Top_p 0.5 Bot_p 0.0\n",
      "\n",
      "Modified Accuracy: 27.47 %\n",
      "Modified Accuracy: 20.35 %\n",
      "Modified Accuracy: 28.20 %\n",
      "Modified Accuracy: 24.83 %\n",
      "Modified Accuracy: 17.72 %\n",
      "Modified Accuracy: 22.60 %\n",
      "Modified Accuracy: 19.04 %\n",
      "Modified Accuracy: 19.28 %\n",
      "Modified Accuracy: 23.79 %\n",
      "Modified Accuracy: 24.64 %\n",
      "\n",
      "Number of loops: 10\n",
      "Average mod acc is: 22.79 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the quantized weights for fc (fc1) layer ----------------------------------------------------------------------------\n",
    "\n",
    "PATH = './sample_models/cifar_weight_2_1e-3.pth'\n",
    "print(PATH)\n",
    "print(\"\")\n",
    "\n",
    "# print(quant_net)  # Can see here that only the linear layers have been quantized\n",
    "\n",
    "fc_layers = [\"fc1._packed_params.weight\", \"fc2._packed_params.weight\", \"fc3._packed_params.weight\"]\n",
    "\n",
    "bit_values_least = [254, 253, 251, 247]\n",
    "bit_values_most  = [239, 223, 191, 127]\n",
    "\n",
    "top_p = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Load in unmodded for comparison ----------------------------------------------------------------------------------------------\n",
    "unmod_quant_net = Net(train_drop1=0, train_drop2=0, train_drop3=0, train_drop4=0, train_drop5=0)\n",
    "unmod_quant_net.load_state_dict(torch.load(PATH))\n",
    "unmod_quant_net = torch.quantization.quantize_dynamic(unmod_quant_net, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "correct = 0\n",
    "total = 0 \n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = unmod_quant_net(images, False, drop1=0, drop2=0, drop3=0, drop4=0, drop5=0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Unmodded Model Accuracy: %.2f %%' % (100 * correct / float(total)))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(top_p)):\n",
    "    print(\"Top_p\", top_p[j], \"Bot_p\", 1-top_p[j]-0.5)\n",
    "    print(\"\")\n",
    "    ave_sum = 0\n",
    "    loops = 10\n",
    "    \n",
    "    for k in range(loops):\n",
    "        \n",
    "        quant_net = Net(train_drop1=0, train_drop2=0, train_drop3=0, train_drop4=0, train_drop5=0)\n",
    "        quant_net.load_state_dict(torch.load(PATH))\n",
    "        quant_net = torch.quantization.quantize_dynamic(quant_net, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
    "    \n",
    "        for i in range(len(fc_layers)):\n",
    "        #     print(fc_layers[i])\n",
    "        #     print(\"\")\n",
    "\n",
    "            fc_weights = quant_net.state_dict()[fc_layers[i]]\n",
    "            fc_weights_int = quant_net.state_dict()[fc_layers[i]].int_repr()\n",
    "\n",
    "            scale = fc_weights.q_scale()\n",
    "            zero_pt = fc_weights.q_zero_point()\n",
    "\n",
    "            # ------------------------------------Insert some bit flipping stuff here ------------------------------------------------\n",
    "            fc_s = fc_weights.size()\n",
    "            random_arr = torch.rand(fc_s)\n",
    "\n",
    "            # convert based on threshold value (value determines how often bit is flipped from 1-0)\n",
    "            threshold_v = 0.5\n",
    "            threshold = 1 - threshold_v\n",
    "            \n",
    "            top_flip = top_p[j]\n",
    "            bot_flip = 1 - top_flip - threshold\n",
    "            \n",
    "            t1 = 1 - top_flip/4\n",
    "            t2 = 1 - 2*top_flip/4\n",
    "            t3 = 1 - 3*top_flip/4\n",
    "            t4 = 1 - 4*top_flip/4\n",
    "            \n",
    "            t1_2 = t4 - bot_flip/4\n",
    "            t2_2 = t4 - 2*bot_flip/4\n",
    "            t3_2 = t4 - 3*bot_flip/4\n",
    "            t4_2 = t4 - 4*bot_flip/4\n",
    "            \n",
    "            random_arr[random_arr > t1] = bit_values_most[0]\n",
    "            random_arr[(random_arr > t2) & (random_arr <= t1)] = bit_values_most[1]\n",
    "            random_arr[(random_arr > t3) & (random_arr <= t2)] = bit_values_most[2]\n",
    "            random_arr[(random_arr > t4) & (random_arr <= t3)] = bit_values_most[3]\n",
    "            \n",
    "            random_arr[(random_arr > t1_2) & (random_arr <= t4)] = bit_values_least[0]\n",
    "            random_arr[(random_arr > t2_2) & (random_arr <= t1_2)] = bit_values_least[1]\n",
    "            random_arr[(random_arr > t3_2) & (random_arr <= t2_2)] = bit_values_least[2]\n",
    "            random_arr[(random_arr > t4_2) & (random_arr <= t3_2)] = bit_values_least[3]\n",
    "            \n",
    "            random_arr[random_arr <= threshold] = 255\n",
    "            random_arr_np = random_arr.type(torch.uint8)\n",
    "\n",
    "            new_fc = fc_weights_int.type(torch.uint8)\n",
    "\n",
    "            new_arr = torch.bitwise_and(random_arr_np, new_fc).type(torch.int8)\n",
    "\n",
    "\n",
    "            # --------------------------------Reload new values (or same ones) into the network again --------------------------------\n",
    "\n",
    "            manual = (new_arr-zero_pt)*scale\n",
    "\n",
    "            new_quant_tensor = torch.quantize_per_tensor(manual, scale, zero_pt, torch.qint8)\n",
    "\n",
    "            state_dict = quant_net.state_dict()\n",
    "            state_dict[fc_layers[i]] = new_quant_tensor\n",
    "            quant_net.load_state_dict(state_dict)\n",
    "\n",
    "            c1_w_params = quant_net.state_dict()[fc_layers[i]]\n",
    "\n",
    "\n",
    "        # Now check accuracy ---------------------------------------------------------------------------------------------\n",
    "\n",
    "        correct = 0\n",
    "        total = 0 \n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                outputs = quant_net(images, False, drop1=0, drop2=0, drop3=0, drop4=0, drop5=0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print('Modified Accuracy: %.2f %%' % (100 * correct / float(total)))\n",
    "    \n",
    "        ave_sum += 100 * correct / float(total)\n",
    "        \n",
    "    print(\"\")\n",
    "    print('Number of loops:', loops)\n",
    "    print('Average mod acc is: %.2f %%' % (ave_sum/float(loops)))\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
